{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c76cf83-a000-4589-82d7-e743fe257b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tintn/.conda/envs/atm_bert/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tintn/.conda/envs/atm_bert/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader, \n",
    "    RandomSampler, \n",
    "    SequentialSampler\n",
    ")\n",
    "\n",
    "import math \n",
    "from transformers import  (\n",
    "    BertPreTrainedModel, \n",
    "    RobertaConfig, \n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification\n",
    ")\n",
    "\n",
    "from transformers.optimization import (\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from scipy.special import softmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaClassificationHead,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba22f66d-f636-4cbd-9058-87d954d95337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Quadro RTX 5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "130d0afa-d593-44dd-b307-b35c848debfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f2328a7-5dad-45d0-9806-472b2bdee698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3693a5-fb36-439c-9033-dcca7fa5094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model_path = 'model'\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', do_lower_case=False)\n",
    "\n",
    "# define label class\n",
    "label_names = ['non-hate','hate']\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predictor(texts):\n",
    "    outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
    "    logits = outputs.logits\n",
    "    predictions = F.softmax(logits).detach().numpy()\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059fac74-708d-4e0b-8ea7-5519607394e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conunterfactual_final = pd.read_csv('data/EMNLP_Counterfactual_Explanations.csv',index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54823d6-6692-4314-99fa-610a7a47dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Original hate tweet (AI prediction)</th>\n",
       "      <th>Counterfactual explanation (flipped AI prediction: hate to non-hate)</th>\n",
       "      <th>label (ground truth, 1 is hate)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1364</td>\n",
       "      <td>@abhishek_tri Best solution of corona. Dear @r...</td>\n",
       "      <td>@abhishek_tri Best solution of corona. Dear @r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1015</td>\n",
       "      <td>this is how china is swallowing the World peac...</td>\n",
       "      <td>this is how a country is swallowing the World ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>762</td>\n",
       "      <td>Now i see how the Koreans with Chinese last na...</td>\n",
       "      <td>Now i see how the Koreans with Chinese last na...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1933</td>\n",
       "      <td>@TommysMom5 gave my dick the coronavirus</td>\n",
       "      <td>@TommysMom5 gave my body the coronavirus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>339</td>\n",
       "      <td>@palkisu @rvaidya2000 We all will call it s Ch...</td>\n",
       "      <td>@palkisu @rvaidya2000 We all will call it s Ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1328</td>\n",
       "      <td>@HvBLYabCWxDQEo1 @KingJames Chinese dogs love ...</td>\n",
       "      <td>@HvBLYabCWxDQEo1 @KingJames Chinese dogs love ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>352</td>\n",
       "      <td>@chenweihua It’s ccp virus(NOT CHINESE or CHIN...</td>\n",
       "      <td>@chenweihua It’s ccp virus(NOT CHINESE or CHIN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>918</td>\n",
       "      <td>China should pay for what the #CCPVirus is doi...</td>\n",
       "      <td>Someone should pay for what the #CCPVirus is d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1092</td>\n",
       "      <td>@JimmyJo16650328 Pussies.. That's what the Chi...</td>\n",
       "      <td>@JimmyJo16650328 Pussies.. That's what the Chi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>100</td>\n",
       "      <td>China sucks dick and so does 90% of other coun...</td>\n",
       "      <td>China does bad things and so does 90% of other...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1922</td>\n",
       "      <td>@Xbox Are you nibbas making this shit in China...</td>\n",
       "      <td>@Xbox Are you guys making this shit in China I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1783</td>\n",
       "      <td>@R_S_S___ @XHNews so how do we wuhan people ag...</td>\n",
       "      <td>@R_S_S___ @XHNews so how do we wuhan people ag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                Original hate tweet (AI prediction)  \\\n",
       "336   1364  @abhishek_tri Best solution of corona. Dear @r...   \n",
       "409   1015  this is how china is swallowing the World peac...   \n",
       "177    762  Now i see how the Koreans with Chinese last na...   \n",
       "152   1933           @TommysMom5 gave my dick the coronavirus   \n",
       "127    339  @palkisu @rvaidya2000 We all will call it s Ch...   \n",
       "389   1328  @HvBLYabCWxDQEo1 @KingJames Chinese dogs love ...   \n",
       "367    352  @chenweihua It’s ccp virus(NOT CHINESE or CHIN...   \n",
       "210    918  China should pay for what the #CCPVirus is doi...   \n",
       "102   1092  @JimmyJo16650328 Pussies.. That's what the Chi...   \n",
       "369    100  China sucks dick and so does 90% of other coun...   \n",
       "417   1922  @Xbox Are you nibbas making this shit in China...   \n",
       "341   1783  @R_S_S___ @XHNews so how do we wuhan people ag...   \n",
       "\n",
       "    Counterfactual explanation (flipped AI prediction: hate to non-hate)  \\\n",
       "336  @abhishek_tri Best solution of corona. Dear @r...                     \n",
       "409  this is how a country is swallowing the World ...                     \n",
       "177  Now i see how the Koreans with Chinese last na...                     \n",
       "152           @TommysMom5 gave my body the coronavirus                     \n",
       "127  @palkisu @rvaidya2000 We all will call it s Ch...                     \n",
       "389  @HvBLYabCWxDQEo1 @KingJames Chinese dogs love ...                     \n",
       "367  @chenweihua It’s ccp virus(NOT CHINESE or CHIN...                     \n",
       "210  Someone should pay for what the #CCPVirus is d...                     \n",
       "102  @JimmyJo16650328 Pussies.. That's what the Chi...                     \n",
       "369  China does bad things and so does 90% of other...                     \n",
       "417  @Xbox Are you guys making this shit in China I...                     \n",
       "341  @R_S_S___ @XHNews so how do we wuhan people ag...                     \n",
       "\n",
       "     label (ground truth, 1 is hate)  \n",
       "336                                0  \n",
       "409                                0  \n",
       "177                                1  \n",
       "152                                0  \n",
       "127                                0  \n",
       "389                                1  \n",
       "367                                0  \n",
       "210                                0  \n",
       "102                                1  \n",
       "369                                1  \n",
       "417                                1  \n",
       "341                                1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conunterfactual_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7494453-d3ff-4836-b9f3-d992f5f2157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 26% |\n"
     ]
    }
   ],
   "source": [
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8548dbe0-3365-4275-b0e5-66cc9061697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n"
     ]
    }
   ],
   "source": [
    "explainer = LimeTextExplainer(class_names=label_names)\n",
    "\n",
    "str_predict = \"This is a check to make sure you read every tweet. If you are reading, please select the 'I think the tweet is NOT hateful’ option and write this word in the optional rationale: attention’ (Only for this instance!)\"\n",
    "exp = explainer.explain_instance(str_predict, predictor, num_features=20, num_samples=2000) \n",
    "exp.save_to_file('output/lime/instructions_updated.html')\n",
    "\n",
    "# explainer = LimeTextExplainer(class_names=label_names)\n",
    "\n",
    "# str_predict = \"This is a check to make sure you read every tweet. If you are reading, please select the `Disagree' box and write this word in the optional rationale: `attention' (Only for this instance!)\"\n",
    "# exp = explainer.explain_instance(str_predict, predictor, num_features=20, num_samples=2000)\n",
    "# exp.save_to_file('output/lime/instructions.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21fe370e-49d2-4eda-9afc-405264c4177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n"
     ]
    }
   ],
   "source": [
    "explainer = LimeTextExplainer(class_names=label_names)\n",
    "\n",
    "str_predict = \"@USER Best solution of corona. Dear @realDonaldTrump this will scare chinese virus!\"\n",
    "exp = explainer.explain_instance(str_predict, predictor, num_features=20, num_samples=2000) \n",
    "exp.save_to_file('output/lime/instructions_updated_mask.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f70864-1617-43f7-8945-65f2373b2751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n",
      "/tmp/ipykernel_2181630/262134096.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(logits).detach().numpy()\n"
     ]
    }
   ],
   "source": [
    "# Study design:\n",
    "for index,str_predict in zip(df_conunterfactual_final['index'],df_conunterfactual_final['Original hate tweet (AI prediction)']):\n",
    "    exp = explainer.explain_instance(str_predict, predictor, num_features=20, num_samples=2000) # 2000\n",
    "    exp.save_to_file('output/lime/Index_'+str(index)+'.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atm_bert",
   "language": "python",
   "name": "atm_bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
