{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203eb65c-b6b6-42da-bd93-a25b319b470c",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91a265c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tintn/.conda/envs/atm_bert/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tintn/.conda/envs/atm_bert/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader, \n",
    "    RandomSampler, \n",
    "    SequentialSampler\n",
    ")\n",
    "\n",
    "import math \n",
    "from transformers import  (\n",
    "    BertPreTrainedModel, \n",
    "    RobertaConfig, \n",
    "    RobertaTokenizerFast\n",
    ")\n",
    "\n",
    "from transformers.optimization import (\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from scipy.special import softmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaClassificationHead,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d681b488-63d3-43df-9e17-701255ad7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asonam_release_all_tweets = pd.read_csv('data/asonam_release_all_tweets.csv')\n",
    "annotated_tweets_w_text = pd.read_csv('data/annotated_tweets_w_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709d175a-f221-438d-ae65-f62c576c5182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1242553623260868608</td>\n",
       "      <td>Are we still allowed to quote ancient Chinese ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1246508137638580225</td>\n",
       "      <td>@mamacat2u @VBeltiz More power to you!  This C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233468243534372865</td>\n",
       "      <td>CNBC: WHO, Tedros reiterated that the virus co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1243626072387747841</td>\n",
       "      <td>\"The heightened racism experienced by Asian co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1225611530978217989</td>\n",
       "      <td>Coronavirus and Nepali in China: KP Oli has di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet ID                                               Text  \\\n",
       "0  1242553623260868608  Are we still allowed to quote ancient Chinese ...   \n",
       "1  1246508137638580225  @mamacat2u @VBeltiz More power to you!  This C...   \n",
       "2  1233468243534372865  CNBC: WHO, Tedros reiterated that the virus co...   \n",
       "3  1243626072387747841  \"The heightened racism experienced by Asian co...   \n",
       "4  1225611530978217989  Coronavirus and Nepali in China: KP Oli has di...   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_tweets_w_text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a6923f-6404-4d9b-9977-a4c34c0de6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6003796066304</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1230768059503140869</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1230773097868873731</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1230777259440496640</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1230766478275665920</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1230772376775692289</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         6003796066304    0\n",
       "0  1230768059503140869  0.0\n",
       "1  1230773097868873731  0.0\n",
       "2  1230777259440496640  0.0\n",
       "3  1230766478275665920  0.0\n",
       "4  1230772376775692289  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asonam_release_all_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60fa5370-8aac-40aa-a18c-ad2cb0587b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_tweets_w_text['label'] = np.where(annotated_tweets_w_text['label']==2,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8803d78-3a8a-46c1-943e-689ecbdf263b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.812664\n",
       "1    0.187336\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_tweets_w_text['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c93d9d-b75f-496b-980a-9cd71b01e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data sets into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(annotated_tweets_w_text['Text'],\n",
    "                                                    annotated_tweets_w_text['label'],\n",
    "                                                    stratify=annotated_tweets_w_text['label'], \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ce24a-0b43-4450-b3fb-55d014b05918",
   "metadata": {},
   "source": [
    "# Define parameters for the fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00e0259-19d2-425e-af14-1dc1bb0afe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Quadro RTX 5000\n"
     ]
    }
   ],
   "source": [
    "# We first check if GPU is available or not\n",
    "print(torch.__version__)\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b42a79d-cef6-4902-98e4-bae9bfddc006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first define the parameters\n",
    "model_name = 'roberta-base'\n",
    "num_labels = 2\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer_name = model_name\n",
    "max_seq_length = 128  \n",
    "train_batch_size = 16\n",
    "test_batch_size = 16\n",
    "weight_decay=0.01\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 10\n",
    "learning_rate = 1e-05\n",
    "adam_epsilon = 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897c5f88-09f2-4749-982e-31fd30ea96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaForSequenceClassification(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(RobertaForSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.roberta(input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186cf789-5f3f-451f-874b-2d26c56792c5",
   "metadata": {},
   "source": [
    "# Load pre-trained Roberta model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19704d3b-8362-4ce1-b77a-a2cae4b35a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 499M/499M [00:39<00:00, 12.7MB/s] \n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'roberta.pooler.dense.weight', 'classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model=\n",
      " RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "Tokenizer= RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_class = RobertaConfig\n",
    "model_class = RobertaForSequenceClassification\n",
    "tokenizer_class = RobertaTokenizerFast\n",
    "\n",
    "config = config_class.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "model = model_class.from_pretrained(model_name, config=config)\n",
    "print('Model=\\n',model,'\\n')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "print('Tokenizer=',tokenizer,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd2e5d-99c3-4ed1-97eb-75ee4b1ec1ee",
   "metadata": {},
   "source": [
    "# Define a class to convert text and labels into a Dataset object with encoded text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26c2b347-2787-451e-826c-1d871655c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer):\n",
    "        text, labels = data\n",
    "        self.examples = tokenizer(text=text,text_pair=None,truncation=True,padding=\"max_length\",\n",
    "                                  max_length=max_seq_length,return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]\n",
    "\n",
    "\n",
    "train_examples = (X_train.astype(str).tolist(), y_train.tolist())\n",
    "train_dataset = ClassificationDataset(train_examples,tokenizer)\n",
    "\n",
    "test_examples = (X_test.astype(str).tolist(),  y_test.tolist())\n",
    "test_dataset = ClassificationDataset(test_examples,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fc37d-d649-44b7-9a8f-d022c27bee98",
   "metadata": {},
   "source": [
    "# Methods to prepare a batch from train (and test) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb8f5d9-12b6-4468-b33a-29201bb9f84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  1039, 10567,  ...,     1,     1,     1],\n",
      "        [    0,  1039,   510,  ...,     1,     1,     1],\n",
      "        [    0, 20763,  1603,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  1106,    47,  ...,     1,     1,     1],\n",
      "        [    0,   250,  7159,  ...,     1,     1,     1],\n",
      "        [    0,  6785,    10,  ...,     1,     1,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "def generate_inputs_dict(batch):\n",
    "    inputs = {key: value.squeeze(1).to(device) for key, value in batch[0].items()}\n",
    "    inputs[\"labels\"] = batch[1].to(device)\n",
    "    return inputs\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=train_batch_size)\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)\n",
    "\n",
    "# Extract a batch as sanity-check\n",
    "batch = generate_inputs_dict(next(iter(train_dataloader)))\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9f5b2c1-3045-4df9-82b0-3b81397cdaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tintn/.conda/envs/atm_bert/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer_grouped_parameters = []\n",
    "custom_parameter_names = set()\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters.extend(\n",
    "    [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "warmup_steps = 500 # warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f137a-cef3-48da-a499-e54a49208648",
   "metadata": {},
   "source": [
    "# Method to compute accuracy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9dc37fd-6985-4831-941c-0b6928319c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, model_outputs, labels, eval_examples=None, multi_label=False):\n",
    "    assert len(preds) == len(labels)\n",
    "    mismatched = labels != preds\n",
    "    wrong = [i for (i, v) in zip(eval_examples, mismatched) if v.any()]\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n",
    "    precision = (tp)/(tp + fp)\n",
    "    recall = (tp)/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision + recall)\n",
    "    scores = np.array([softmax(element)[1] for element in model_outputs])\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    auprc = average_precision_score(labels, scores)\n",
    "    return (\n",
    "        {\n",
    "            **{\"mcc\": mcc, \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn, \"auroc\": auroc, \"auprc\": auprc, \"precision\": precision, \"recall\": recall, \"f1\": f1},\n",
    "        },\n",
    "        wrong,\n",
    "        scores\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2ea2ed4-bf99-475f-aa03-d73b6344e02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>711</td>\n",
       "      <td>#coronavirus #COVID19 You can't even say goodb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>517</td>\n",
       "      <td>The President is knowingly inciting hate again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1652</td>\n",
       "      <td>@ChenXiHao Don‚Äôt discriminate us Asians ! üíôüôèüèª ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1502</td>\n",
       "      <td>it breaks my heart that you fucking racists ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512</td>\n",
       "      <td>@SkyNews @Independent @Channel4News @itvnews @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1246</td>\n",
       "      <td>Lol, shut your ass up you fat covid 19 virus h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>615</td>\n",
       "      <td>After Corona virus in China, now it's Pakistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>63</td>\n",
       "      <td>@Jay_Qi37__China @EnzoMazak @eille68577793 @an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>529</td>\n",
       "      <td>@globaltimesnews Fuck China! #ChineseBioterror...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>1687</td>\n",
       "      <td>With the global outbreak of Coronavirus disrup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               Text\n",
       "0      711  #coronavirus #COVID19 You can't even say goodb...\n",
       "1      517  The President is knowingly inciting hate again...\n",
       "2     1652  @ChenXiHao Don‚Äôt discriminate us Asians ! üíôüôèüèª ...\n",
       "3     1502  it breaks my heart that you fucking racists ar...\n",
       "4      512  @SkyNews @Independent @Channel4News @itvnews @...\n",
       "..     ...                                                ...\n",
       "453   1246  Lol, shut your ass up you fat covid 19 virus h...\n",
       "454    615  After Corona virus in China, now it's Pakistan...\n",
       "455     63  @Jay_Qi37__China @EnzoMazak @eille68577793 @an...\n",
       "456    529  @globaltimesnews Fuck China! #ChineseBioterror...\n",
       "457   1687  With the global outbreak of Coronavirus disrup...\n",
       "\n",
       "[458 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final =  X_test.reset_index().copy()\n",
    "X_test_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a97419c4-7d4a-437c-a3f7-eda859f698f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2180351/549045081.py:7: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = (tp)/(tp+fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 Training avg loss 0.4605375173299209\n",
      "epoch 0 Testing  avg loss 0.36912028902563554\n",
      "{'mcc': 0.0, 'tp': 0, 'tn': 372, 'fp': 0, 'fn': 86, 'auroc': 0.8910040010002501, 'auprc': 0.6804892784139719, 'precision': nan, 'recall': 0.0, 'f1': nan}\n",
      "epoch 1 Training avg loss 0.32475626215986586\n",
      "epoch 1 Testing  avg loss 0.32061040439996225\n",
      "{'mcc': 0.5362980289990233, 'tp': 31, 'tn': 370, 'fp': 2, 'fn': 55, 'auroc': 0.9233870967741935, 'auprc': 0.7997807095987548, 'precision': 0.9393939393939394, 'recall': 0.36046511627906974, 'f1': 0.5210084033613445}\n",
      "epoch 2 Training avg loss 0.2351519814328007\n",
      "epoch 2 Testing  avg loss 0.2215027079500001\n",
      "{'mcc': 0.7209090086804603, 'tp': 60, 'tn': 361, 'fp': 11, 'fn': 26, 'auroc': 0.9467679419854964, 'auprc': 0.8633869023931652, 'precision': 0.8450704225352113, 'recall': 0.6976744186046512, 'f1': 0.7643312101910827}\n",
      "epoch 3 Training avg loss 0.1551587715583003\n",
      "epoch 3 Testing  avg loss 0.20180056629509763\n",
      "{'mcc': 0.7834318747275633, 'tp': 73, 'tn': 354, 'fp': 18, 'fn': 13, 'auroc': 0.9507064266066516, 'auprc': 0.8767974820257599, 'precision': 0.8021978021978022, 'recall': 0.8488372093023255, 'f1': 0.8248587570621468}\n",
      "epoch 4 Training avg loss 0.10388338803921057\n",
      "epoch 4 Testing  avg loss 0.23684544583525638\n",
      "{'mcc': 0.7307212804816817, 'tp': 62, 'tn': 360, 'fp': 12, 'fn': 24, 'auroc': 0.9454863715928983, 'auprc': 0.868920736625987, 'precision': 0.8378378378378378, 'recall': 0.7209302325581395, 'f1': 0.7749999999999999}\n",
      "epoch 5 Training avg loss 0.05491827524307629\n",
      "epoch 5 Testing  avg loss 0.2778784277335066\n",
      "{'mcc': 0.750651048686889, 'tp': 66, 'tn': 358, 'fp': 14, 'fn': 20, 'auroc': 0.9492060515128782, 'auprc': 0.8567966375188082, 'precision': 0.825, 'recall': 0.7674418604651163, 'f1': 0.7951807228915662}\n",
      "epoch 6 Training avg loss 0.03451865777049375\n",
      "epoch 6 Testing  avg loss 0.2984611231046889\n",
      "{'mcc': 0.7323119145820267, 'tp': 63, 'tn': 359, 'fp': 13, 'fn': 23, 'auroc': 0.9380470117529383, 'auprc': 0.8590318203125942, 'precision': 0.8289473684210527, 'recall': 0.7325581395348837, 'f1': 0.7777777777777778}\n",
      "epoch 7 Training avg loss 0.024213115196756046\n",
      "epoch 7 Testing  avg loss 0.3254535231209392\n",
      "{'mcc': 0.7220970655479835, 'tp': 67, 'tn': 352, 'fp': 20, 'fn': 19, 'auroc': 0.9384533633408352, 'auprc': 0.8553011849622226, 'precision': 0.7701149425287356, 'recall': 0.7790697674418605, 'f1': 0.7745664739884394}\n",
      "epoch 8 Training avg loss 0.017841340530582745\n",
      "epoch 8 Testing  avg loss 0.32616874335558504\n",
      "{'mcc': 0.7317261187126738, 'tp': 66, 'tn': 355, 'fp': 17, 'fn': 20, 'auroc': 0.9386409102275569, 'auprc': 0.8545914133338225, 'precision': 0.7951807228915663, 'recall': 0.7674418604651163, 'f1': 0.7810650887573964}\n",
      "epoch 9 Training avg loss 0.01684486972492026\n",
      "epoch 9 Testing  avg loss 0.32616874335558504\n",
      "{'mcc': 0.7317261187126738, 'tp': 66, 'tn': 355, 'fp': 17, 'fn': 20, 'auroc': 0.9386409102275569, 'auprc': 0.8545914133338225, 'precision': 0.7951807228915663, 'recall': 0.7674418604651163, 'f1': 0.7810650887573964}\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = generate_inputs_dict(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "    # evaluate model after each epoch\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(test_dataloader)\n",
    "    preds = np.empty((len(test_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(test_dataset)))\n",
    "    model.eval()\n",
    "    \n",
    "    for i,test_batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_batch = generate_inputs_dict(test_batch)\n",
    "            input_ids = test_batch['input_ids'].to(device)\n",
    "            attention_mask = test_batch['attention_mask'].to(device)\n",
    "            labels = test_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result, wrong, scores = compute_metrics(preds, model_outputs, out_label_ids, test_examples)\n",
    "    X_test_final['preds'] = preds\n",
    "    X_test_final['scores'] = scores # predicting 1\n",
    "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "    print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0093f5e0-349c-4ae9-b8e3-2dae598af779",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "269362df-4e00-4e20-9bb5-a19a72402a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final['label'] = y_test.values\n",
    "X_test_hate = X_test_final[X_test_final['preds']==1].sort_values(['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9285fb16-75ff-43bc-94ee-58fa5c51d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_hate.to_csv('output/X_test_hate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4f41bb6-936a-432b-9ac1-addc5166c99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Text</th>\n",
       "      <th>preds</th>\n",
       "      <th>scores</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>2278</td>\n",
       "      <td>#YoMuzziesSoBad , they h@te Hindus so much, th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2259</td>\n",
       "      <td>@alx @realDonaldTrump No,now we Chinese have b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.510949</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>294</td>\n",
       "      <td>Absolute joke of a man. Cut their salaries off...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2159</td>\n",
       "      <td>@SpicyKoreanQ The world would be a much better...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.527265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>369</td>\n",
       "      <td>Chinese people be weird af... now there‚Äôs goin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.656011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2092</td>\n",
       "      <td>@zlj517 Mother  fucker china virus..what about...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>45</td>\n",
       "      <td>#COVID19 so this all started cuz some chinks w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1618</td>\n",
       "      <td>@yoogieboobie Fuck u and your chink obsession</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997814</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1699</td>\n",
       "      <td>@CNN Fucking chinese virus  Fucking chinese liars</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1963</td>\n",
       "      <td>Man! I hope I don't get that Commie Chinese Wu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               Text  preds  \\\n",
       "425   2278  #YoMuzziesSoBad , they h@te Hindus so much, th...      1   \n",
       "203   2259  @alx @realDonaldTrump No,now we Chinese have b...      1   \n",
       "281    294  Absolute joke of a man. Cut their salaries off...      1   \n",
       "146   2159  @SpicyKoreanQ The world would be a much better...      1   \n",
       "239    369  Chinese people be weird af... now there‚Äôs goin...      1   \n",
       "..     ...                                                ...    ...   \n",
       "115   2092  @zlj517 Mother  fucker china virus..what about...      1   \n",
       "291     45  #COVID19 so this all started cuz some chinks w...      1   \n",
       "205   1618      @yoogieboobie Fuck u and your chink obsession      1   \n",
       "174   1699  @CNN Fucking chinese virus  Fucking chinese liars      1   \n",
       "313   1963  Man! I hope I don't get that Commie Chinese Wu...      1   \n",
       "\n",
       "       scores  label  \n",
       "425  0.506634      0  \n",
       "203  0.510949      0  \n",
       "281  0.524943      0  \n",
       "146  0.527265      1  \n",
       "239  0.656011      1  \n",
       "..        ...    ...  \n",
       "115  0.997734      1  \n",
       "291  0.997773      1  \n",
       "205  0.997814      1  \n",
       "174  0.997878      1  \n",
       "313  0.997897      1  \n",
       "\n",
       "[83 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_hate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
